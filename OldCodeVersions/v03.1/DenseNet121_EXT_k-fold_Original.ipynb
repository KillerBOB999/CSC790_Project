{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Sequential\n",
    "from tensorflow.random import set_seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "# import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeding random state to 13 always, for reproducibility\n",
    "np.random.seed(13)\n",
    "set_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPU, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "### Used to select GPU 0=first device, 1=second device, etc...\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('gpus:',gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function: Return the paths to all jpg files found within a directory\n",
    "def getImageDirs(root: str = \"data\"):\n",
    "    imageDirs = []\n",
    "    for subDirectory, directory, files in os.walk(root):\n",
    "        for file in files:\n",
    "            if file[-4:] == \".jpg\":\n",
    "                path = os.path.join(subDirectory, file)         \n",
    "                imageDirs.append(path)\n",
    "    return(imageDirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function: Return the class weights given a list of classes\n",
    "def getClassWeightsFromLabels(labels: List[int]):# -> Dict[int]:\n",
    "    weights = sklearn.utils.class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "    return {0: weights[0], 1: weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function: Return the img paths and classes in seperate lists given a txt file from the LABELS folder\n",
    "def getDirsAndClasses(root: str, file: str) -> Tuple[List[str], List[int]]:\n",
    "    imageDirs = []\n",
    "    classes = []\n",
    "    line = \"\"\n",
    "    with open(root + file, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            imageDir, clazz = line.split()\n",
    "            imageDirs.append(imageDir)\n",
    "            classes.append(int(clazz))\n",
    "    return imageDirs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function: Create a Keras prebuilt model\n",
    "def makeModel(inputShape: Tuple[int], modelName:str ='') -> keras.Model:\n",
    "    \"\"\"\n",
    "    Source: https://www.tensorflow.org/guide/keras/functional#a_toy_resnet_model\n",
    "    \n",
    "    Note that I tend to prefer the super-explicit (if somewhat verbose) style. \n",
    "    This style is technically unnecessary, but it helps with readability.\n",
    "\n",
    "    Load model by inputing the name to modelName\n",
    "    Options are \"Simple_ResNet\", \"SimpleNet\", \"InceptionResNetV2\", \"MobileNetV2\", \"ResNet50V2\", \"DenseNet121\", \"DenseNet201\", and \"NASNetLarge\"\n",
    "    \"\"\"\n",
    "    input = keras.Input(shape=inputShape, name=\"Input\")\n",
    "    x=None\n",
    "    if modelName == \"Simple_ResNet\":\n",
    "\n",
    "        x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(input)\n",
    "        x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "        block_1_output = layers.MaxPooling2D(pool_size=(3, 3), strides=(3, 3))(x)\n",
    "\n",
    "        x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(block_1_output)\n",
    "        x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "        block_2_output = layers.add([x, block_1_output])\n",
    "\n",
    "        x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(block_2_output)\n",
    "        x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "        block_3_output = layers.add([x, block_2_output])\n",
    "\n",
    "        x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(block_3_output)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    elif modelName in [\"InceptionResNetV2\",\"MobileNetV2\",\"ResNet50V2\",\"DenseNet121\",\"DenseNet201\",\"NASNetLarge\"]:\n",
    "        \n",
    "        if modelName==\"InceptionResNetV2\":\n",
    "            baseModel = keras.applications.InceptionResNetV2(include_top=False, weights=\"imagenet\", input_shape=(150,150,3))(input)\n",
    "        elif modelName==\"MobileNetV2\":\n",
    "            baseModel = keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=(150,150,3))(input)\n",
    "        elif modelName==\"ResNet50V2\":\n",
    "            baseModel = keras.applications.ResNet50V2(include_top=False, weights=\"imagenet\", input_shape=(150,150,3))(input)\n",
    "        elif modelName==\"DenseNet121\":\n",
    "            baseModel = keras.applications.DenseNet121(include_top=False, weights=\"imagenet\", input_shape=(150,150,3))(input)\n",
    "        elif modelName==\"DenseNet201\":\n",
    "            baseModel = keras.applications.DenseNet201(include_top=False, weights=\"imagenet\", input_shape=(150,150,3))(input)\n",
    "        elif modelName==\"NASNetLarge\":\n",
    "            x = tf.keras.layers.experimental.preprocessing.Resizing(height=331, width=331)(input)\n",
    "            baseModel = keras.applications.NASNetLarge(include_top=False, weights=\"imagenet\", input_shape=(331,331,3))(x)\n",
    "        else:\n",
    "            raise(\"Model Name Not In Recognized Keras Models\")\n",
    "        \n",
    "        baseModel.trainable = False\n",
    "        x = layers.Flatten()(baseModel)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    elif modelName==\"SimpleNet\":\n",
    "        x = layers.AveragePooling2D(pool_size=(50, 50))(input)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    if x != None:\n",
    "        output=layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "        return keras.Model(inputs=input, outputs=output, name=modelName)\n",
    "    else:\n",
    "        raise(\"Model Name Not Recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94493it [00:00, 1065564.27it/s]\n",
      "94493it [00:00, 1044370.58it/s]\n",
      "31825it [00:00, 1278689.97it/s]\n",
      "31825it [00:00, 1993722.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get the Train Dataset using split from the LABELS folder\n",
    "root = os.getcwd() + \"/CNR-EXT-Patches-150x150\"\n",
    "imageDirs, classes = getDirsAndClasses(root, \"/LABELS/train.txt\")\n",
    "root = root + \"/PATCHES/\"\n",
    "train = pd.DataFrame([\n",
    "            {\n",
    "                \"image\": root + filename,\n",
    "                \"class\": \"free\" if clazz == 0 else \"busy\"\n",
    "            }\n",
    "            for filename, clazz in tqdm(zip(imageDirs, classes))\n",
    "    ])\n",
    "#Now Get Test\n",
    "root = os.getcwd() + \"/CNR-EXT-Patches-150x150\"\n",
    "imageDirs, classes = getDirsAndClasses(root, \"/LABELS/test.txt\")\n",
    "root = root + \"/PATCHES/\"\n",
    "test = pd.DataFrame([\n",
    "            {\n",
    "                \"image\": root + filename,\n",
    "                \"class\": \"free\" if clazz == 0 else \"busy\"\n",
    "            }\n",
    "            for filename, clazz in tqdm(zip(imageDirs, classes))\n",
    "    ])\n",
    "dataset=train.append(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56842 non-validated image filenames belonging to 2 classes.\n",
      "Found 63159 non-validated image filenames belonging to 2 classes.\n",
      "Found 6317 non-validated image filenames belonging to 2 classes.\n",
      "\n",
      "Found 56842 non-validated image filenames belonging to 2 classes.\n",
      "Found 63159 non-validated image filenames belonging to 2 classes.\n",
      "Found 6317 non-validated image filenames belonging to 2 classes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose number of folds (1=normal experiment)\n",
    "n_folds = 10\n",
    "\n",
    "# Make the k folds\n",
    "kFCV_sets=[]\n",
    "\n",
    "busy_samples=dataset.loc[dataset[\"class\"] == \"busy\"]\n",
    "free_samples=dataset.loc[dataset[\"class\"] == \"free\"]\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "if n_folds != 1:\n",
    "    busy_kf = sklearn.model_selection.KFold(n_splits = n_folds)\n",
    "    free_kf = sklearn.model_selection.KFold(n_splits = n_folds)\n",
    "\n",
    "\n",
    "for k in range(n_folds):\n",
    "\n",
    "    if n_folds != 1:\n",
    "        busy = next(busy_kf.split(busy_samples), None)\n",
    "        free = next(busy_kf.split(free_samples), None)\n",
    "    else:\n",
    "        busy = [i for i in range(int(len(busy_train)*0.9))],[i for i in range(int(len(busy_train)*0.9), len(busy_train))]\n",
    "        free = [i for i in range(int(len(free_train)*0.9))],[i for i in range(int(len(free_train)*0.9), len(free_train))]\n",
    "\n",
    "    busy_train, free_train = busy_samples.iloc[busy[0]], free_samples.iloc[free[0]]\n",
    "    busy_train, busy_val = busy_train[:int(len(busy_train)*0.9)], busy_train[int(len(busy_train)*0.9):]\n",
    "    free_train, free_val = free_train[:int(len(free_train)*0.9)], free_train[int(len(free_train)*0.9):]\n",
    "\n",
    "    train = busy_train.append(free_train)\n",
    "    val = busy_val.append(free_val)\n",
    "    test = busy_samples.iloc[busy[1]].append(free_samples.iloc[free[1]])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #Declare data generators and preprocessing\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        #Augment data with random flips, normalize each sample's input\n",
    "        vertical_flip = True,\n",
    "        horizontal_flip = True,\n",
    "        rescale = 1.0 / 255.0,\n",
    "        samplewise_std_normalization = True\n",
    "    )\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        directory = None, #none since the df has absolute paths\n",
    "        dataframe = train,\n",
    "        x_col = \"image\",\n",
    "        y_col = \"class\",\n",
    "        validate_filenames = False, #faster for huge datasets\n",
    "        target_size = (150, 150),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = 128,\n",
    "        class_mode = \"binary\",\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        samplewise_std_normalization = True\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        directory = None,\n",
    "        dataframe = test,\n",
    "        x_col = \"image\",\n",
    "        y_col = \"class\",\n",
    "        validate_filenames = False,\n",
    "        target_size = (150, 150),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = 128,\n",
    "        class_mode = \"binary\",\n",
    "        shuffle = True\n",
    "    )\n",
    "    val_generator = test_datagen.flow_from_dataframe(\n",
    "        directory = None,\n",
    "        dataframe = val,\n",
    "        x_col = \"image\",\n",
    "        y_col = \"class\",\n",
    "        validate_filenames = False,\n",
    "        target_size = (150, 150),\n",
    "        color_mode = \"rgb\",\n",
    "        batch_size = 128,\n",
    "        class_mode = \"binary\",\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    print()\n",
    "\n",
    "    kFCV_sets.append([train_generator, test_generator, val_generator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare Callbacks: stop training if accuracy doesn't rise 1% within 3 epochs\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = \"val_loss\",\n",
    "        patience = 3,\n",
    "        verbose = 1,\n",
    "        restore_best_weights = True,\n",
    "        min_delta = 0.01\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9585174193113217, 1: 1.045235555882461}\n"
     ]
    }
   ],
   "source": [
    "#Extract Class Weights (Weights will be the same for all folds)\n",
    "classes = list(train[\"class\"])\n",
    "weights_dict = getClassWeightsFromLabels(classes)\n",
    "print(weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 of 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c07a95eccc02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#Build Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DenseNET121\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     Model.compile(\n",
      "\u001b[0;32m<ipython-input-7-d7905761c339>\u001b[0m in \u001b[0;36mmakeModel\u001b[0;34m(inputShape, modelName)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Name Not Recognized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "k_results=pd.DataFrame(columns = ['Fold', 'Loss', 'Accuracy'])\n",
    "\n",
    "for i,k in enumerate(kFCV_sets):\n",
    "\n",
    "    print(\"Fold\",i+1,\"of\",len(kFCV_sets))\n",
    "\n",
    "    #keras.backend.clear_session()\n",
    "    train_generator, test_generator, val_generator = k\n",
    "\n",
    "    #Build Model\n",
    "    Model = makeModel((150, 150, 3), \"DenseNET121\")\n",
    "    opt = tf.optimizers.Adam()\n",
    "    Model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "        metrics = [\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    if i == 0:\n",
    "        Model.summary()\n",
    "\n",
    "    #Fit data  \n",
    "    Model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        callbacks = callbacks,\n",
    "        epochs = 100,\n",
    "        class_weight = weights_dict,\n",
    "        max_queue_size = 1000,\n",
    "        workers = os.cpu_count(),\n",
    "    )\n",
    "\n",
    "    #Test accuracy\n",
    "    results = Model.evaluate(\n",
    "        test_generator,\n",
    "        max_queue_size = 1000,\n",
    "        workers = os.cpu_count(),\n",
    "    )\n",
    "\n",
    "    k_results = k_results.append({'Fold':i+1, 'Loss':results[0], 'Accuracy':results[1]}, ignore_index=True)\n",
    "\n",
    "k_results = k_results.append({'Fold':\"Avg\", 'Loss':np.average(k_results['Loss']), 'Accuracy':np.average(k_results['Accuracy'])}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_folds != 1:\n",
    "    k_results.to_csv(\"Models/DenseNET121/k-fcv_DenseNET121.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "if n_folds == 1:\n",
    "    Model.save(\"Models/DenseNET121\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
